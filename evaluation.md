# System Documentation for Evaluating Arabic Code-Switched Chatbot Output
---

## Problem Statement
Developing a comprehensive strategy for evaluating the quality of Arabic text generated by AI language models (LLMs) with a focus on fluency, coherence, and relevance. The system will be built for Modern Standard Arabic (MSA) and designed to scale to different Arabic dialects.

### Objective
Design and implement a scalable system architecture specifically for evaluating the quality of Arabic text generated by LLMs using key metrics. 

### System Architecture
The architecture consists of several layers that interact with each other to evaluate Arabic text. Below is a detailed description of each layer:

#### 1. Input Layer
   - **Description**: This layer handles raw text input from the LLM for evaluation.
   - **Functionality**: Accepts input in various formats, including plain text and structured JSON.

#### 2. Preprocessing Module
   - **Description**: Prepares the text for analysis, focusing on tokenization without normalization.
   - **Key Task**: Tokenize the input using appropriate methods for Arabic text without altering the original content.

#### 3. Evaluation Metrics Layer
   - **Description**: This layer is composed of various modules to assess the text based on defined metrics across different levels (character, token, word, clause, sentence, paragraph, response, and prompt + reply).
   - **Functionality**: Each module will evaluate the raw text according to specific criteria, which include fluency, coherence, and relevance, without any prior normalization of the output.

   - **Evaluation Levels**:
     - **Character Level**: Assess forms, nuqat, modifiers and ligatures.
     - **Word Level**: Examine syntax, grammar, part of speech, and semantics.
     - **Sentence Level**: Evaluate grammar, fluency, and verbosity.
     - **Paragraph Level**: Assess coherence, entity consistency, readability, and richness.
     - **Response Level**: Evaluate structure, presentation, cohesion, and time.
     - **Prompt + Reply Level**: Assess relevance, completeness, cost, and adherence.

#### 4. Penalty System
   - **Description**: A level-wise component that calculates penalties based on evaluation results.
   - **Functionality**: Provides a level-wise score for the overall quality of the text based on the metrics applied.

#### 5. Output Layer
   - **Description**: Generates reports or summaries of the evaluation based on all aggregated data.
   - **Functionality**: Outputs final scores based on data and insights regarding the evaluated text.

### Tools and Libraries
- **Programming Language**: Python
- **Pre-trained Arabic Language Models**: AraGPT2, AraBERT
- **Evaluation Metrics Libraries**: BLEU, ROUGE
- **Recommended Libraries**:
  - **Farasa**: Tokenization and morphological analysis.
  - **NLTK**: General text processing.
  - **spaCy**: Supports tokenization and NLP tasks.
  - **Transformers**: For leveraging deep learning models.

### Challenges
- **Evaluation Metric Design**: Customizing metrics for the unique features of Arabic.
- **Automation**: Automating comparison of generated text with reference content.
- **Morphology and Dialects**: Addressing complexities in Arabic morphology and dialectal variations.

### Expected Outcome
A scalable, automated system capable of evaluating the quality of Arabic text generation across different dialects while managing challenges associated with dialectal variations.

---

## Input Layer

### Objective:
To handle raw input in the form of a simple JSON object containing a prompt and reply.

### Functionality:
#### 1. Input Validation:
   - Ensure the input is a valid JSON object with both `prompt` and `reply` fields with string keys.
   - Raise a specific error if the format is invalid.

#### 2. Field Extraction:
   - Extract the `prompt` and `reply` from the JSON input for further processing.

#### 3. Non-Empty Check:
   - Validate that both `prompt` and `reply` are non-empty strings.
   - Raise an error if either field is empty.

#### 4. Arabic Text Check:
   - Ensure that the `reply` is composed of Arabic characters only.
   - Check for valid encoding (UTF-8) to handle special Arabic characters correctly.

#### 5. Return Values:
   - Return the `prompt` and `reply` as a tuple for use in subsequent layers.

### Pseudocode:
```sql
FUNCTION process_input(raw_input):

    VERIFY the input is in the correct structured format

    EXTRACT the 'prompt' and 'reply' from the input

    ENSURE both the 'prompt' and 'reply' are non-empty text values

    VALIDATE that the 'reply' contains Arabic text

    CHECK that the text encoding is appropriate for processing

    RETURN the 'prompt' and 'reply' for further use
```
---

## Evaluation Layer: Overview

### Description
The Evaluation Layer is designed to assess the quality of Arabic text generated by AI language models. It evaluates multiple aspects of the text, such as fluency, coherence, and relevance, at different granularities, from characters to paragraphs.

### Asynchronous Evaluation
The layer is structured to perform evaluations asynchronously, allowing multiple evaluation functions to run concurrently. This approach minimizes overall processing time by waiting only for the longest-running evaluation rather than the sum of all individual evaluation times.
  
#### Benefits of Async Processing:
- **Efficiency**: Evaluations can be executed simultaneously, reducing the time taken for the entire evaluation process.
- **Scalability**: The system can handle larger inputs and more complex evaluations without significant delays.
- **Responsiveness**: The system can provide quicker feedback, enhancing the user experience.

### Evaluation Levels
Each evaluation function will operate independently, allowing for modular evaluations:
- **Prompt + Reply**: Validates and evaluates the overall relevance and completeness of the interaction.
- **Response**: Assesses the generated reply for quality metrics.
- **Paragraph**: Assesses the coherence and readability of individual text blocks.
- **Sentence**: Evaluates individual sentences for grammatical correctness and fluency.
- **Word**: Analyzes the choice of words, including syntax and semantics.
- **Character**: Looks at the quality of characters, forms, diacritics, and ligatures.

### Pseudocode
```sql
FUNCTION evaluate_prompt_reply(reply, context):
    PROCESS the reply using the specified context of the prompt
    GATHER evaluation metrics from the processed reply
    STORE the results in the database

FUNCTION evaluate_response(response):
    PROCESS the response independently
    GATHER evaluation metrics from the processed response
    STORE the results in the database

FUNCTION evaluate_paragraphs(response):
    DIVIDE the response into individual paragraphs
    FOR each paragraph in the divided response:
        COLLECT metrics by evaluating the paragraph
        AGGREGATE metrics for final analysis

    COMPUTE average metrics from aggregated results
    STORE the average metrics in the database

FUNCTION evaluate_sentences(paragraph):
    DIVIDE the paragraph into sentences
    FOR each sentence in the divided paragraph:
        COLLECT metrics by evaluating the sentence
        AGGREGATE metrics for final analysis

    COMPUTE average metrics from aggregated results
    STORE the average metrics in the database

FUNCTION evaluate_words(sentence):
    DIVIDE the sentence into words
    FOR each word in the divided sentence:
        COLLECT metrics by evaluating the word
        AGGREGATE metrics for final analysis

    COMPUTE average metrics from aggregated results
    STORE the average metrics in the database

FUNCTION evaluate_characters(word):
    DIVIDE the word into characters
    FOR each character in the divided word:
        COLLECT metrics by evaluating the character
        AGGREGATE metrics for final analysis

    COMPUTE average metrics from aggregated results
    STORE the average metrics in the database
```

### Final Notes
The asynchronous nature of the Evaluation Layer allows for efficient and quick assessments of text quality, while proper error handling ensures that the system remains robust and resilient against evaluation failures.

---

## Evaluation Metrics: I/O Level

### Description
This section evaluates the interaction between the prompt and the generated reply by assessing metrics such as relevance, completeness, cost, and adherence to ensure the response aligns with user expectations and requirements*

### 1. Relevance
*This metric evaluates how closely the generated reply corresponds to the intent of the prompt, ensuring contextually appropriate responses.*

#### Objective
To assess how well the generated reply corresponds to the intent of the prompt, ensuring the response is contextually appropriate.

#### Implementation Details
- **Approach**:
  - Utilize semantic similarity metrics to compare the prompt and reply embeddings.
  - Common techniques include cosine similarity with pre-trained models like Sentence-BERT, which is particularly effective for sentence-level semantic similarity.

- **Libraries/Modules**:
  - **Transformers**: For accessing pre-trained models.
  - **SciPy**: For calculating cosine similarity.
  - **Sentence-Transformers**: A library specifically designed for working with Sentence-BERT models.

#### Pseudocode:
```sql
FUNCTION calculate_relevance(prompt, reply):
    EMBED the prompt using Transformer model
    EMBED the reply using Transformer model
    COMPUTE relevance score using cosine similarity between embeddings
    RETURN relevance score
```

### 2. Completeness
*This metric measures the extent to which the reply encompasses essential elements and entities present in the prompt.*

#### Objective
To evaluate the extent to which the reply covers the essential elements and entities present in the prompt.

#### Implementation Details
- **Approach**:
  - Use Named Entity Recognition (NER) to extract relevant entities from both the prompt and reply.
  - Measure the overlap of these entities to compute a completeness score.

- **Libraries/Modules**:
  - **spaCy**: For NER and text processing.
  - **NLTK**: For general text processing tasks, including tokenization.

#### Pseudocode:
```sql
FUNCTION calculate_completeness(prompt, reply):
    LOAD NER model
    EXTRACT entities from the prompt using NER
    EXTRACT entities from the reply using NER
    COMPUTE completeness score based on entity overlap
    RETURN completeness score

FUNCTION calculate_overlap(entities_prompt, entities_reply):
    CONVERT entities_prompt to a set
    CONVERT entities_reply to a set
    FIND overlap between sets
    RETURN normalized overlap based on the number of prompt entities
```

### 3. Cost
*This metric computes the total resource cost associated with processing the input and generating the output, aiding in resource utilization analysis.*

#### Objective
To compute the total cost associated with processing the input and generating the output, which can help in analyzing resource utilization.

#### Implementation Details
- **Approach**:
  - Track the processing time and resource usage during the generation of the reply.
  - Sum the input cost (if applicable) and output cost.

- **Libraries/Modules**:
  - **time**: For measuring execution time.
  - **resource**: To track memory usage (if needed).
  - **logging**: For storing cost-related data for future reference.

#### Pseudocode:
```sql
FUNCTION calculate_cost(input_cost, output_cost):
    CALCULATE total cost by summing input cost and output cost
    LOG cost data for future reference
    RETURN total cost

FUNCTION log_cost_data(input_cost, output_cost, total_cost):
    RECORD input cost, output cost, and total cost in logs
```
---

### 4. Adherence  
*This metric evaluates the response's compliance with user instructions and system guidelines, ensuring it aligns with expected standards while resisting manipulative attempts.*  

#### Objective  
To assess the system's capability to follow predefined guidelines and effectively resist prompt injections. This involves evaluating the reply's adherence to core instructions and robustness against user manipulations aimed at influencing the generated content.

#### Implementation Details  
- **Approach**:  
  - Examine the reply for potential indicators of prompt injections or attempts to circumvent system constraints.  
  - Assess the generated response against a defined set of guidelines or rules that determine acceptable content, ensuring the reply remains within these parameters.

- **Libraries/Modules**:  
  - **re**: For regular expression matching to identify potential prompt injection patterns.  
  - **NLTK**: For text processing, including tokenization and identifying problematic phrases.  
  - **spaCy**: For advanced linguistic analysis to detect compliance with guidelines.

#### Pseudocode:  
```sql
FUNCTION calculate_adherence(prompt, reply, guidelines):
    injection_patterns = extract_injection_patterns(guidelines)

    IF contains_injection(reply, injection_patterns):
        RETURN 0.0  # No adherence due to prompt injection

    adherence_score = evaluate_guideline_compliance(reply, guidelines)
    RETURN adherence_score  # Score between 0 (non-compliant) and 1 (fully compliant)

FUNCTION extract_injection_patterns(guidelines):
    RETURN create_patterns_based_on(guidelines)  # Generate patterns that signify injection attempts

FUNCTION contains_injection(reply, injection_patterns):
    FOR each pattern IN injection_patterns:
        IF pattern detected in reply:
            RETURN True  # Injection detected
    RETURN False  # No injection found

FUNCTION evaluate_guideline_compliance(reply, guidelines):
    compliance_score = compare_reply_with_guidelines(reply, guidelines)
    RETURN compliance_score  # Normalize to a scale from 0 to 1
```
--- 

## Evaluation Metrics: Response Level

### Description
This section evaluates the overall quality and effectiveness of the generated response by assessing metrics such as structure, presentation, cohesion, and time taken to generate the response. These metrics are independent of the prompt and assess whether the response is engaging and coherent.

### 1. Structure
*This metric assesses the logical flow and organization of the response, ensuring that it follows a coherent structure that is easy for users to understand.*

#### Objective
To evaluate how well the response is organized and structured, ensuring clarity and logical flow.

#### Implementation Details
- **Approach**:
  - Analyze sentence structure by evaluating transitions and paragraphing.
  - Assess whether the response includes clear topic sentences and appropriate use of paragraphs.

- **Libraries/Modules**:
  - **spaCy**: For syntactic analysis and sentence parsing.
  - **NLTK**: For text processing and coherence evaluation.

#### Pseudocode:
```sql
FUNCTION evaluate_structure(reply):
    SPLIT reply into paragraphs
    COUNT number of paragraphs
    CHECK topic sentences in paragraphs
    
    CALCULATE structure score based on topic sentence presence and paragraph count
    RETURN structure score

FUNCTION split_into_paragraphs(reply):
    RETURN reply.split("\n")

FUNCTION count_paragraphs(paragraphs):
    RETURN len(paragraphs)

FUNCTION check_topic_sentences(paragraphs):
    topic_sentence_count = 0
    FOR each paragraph IN paragraphs:
        IF first sentence is a topic sentence:
            INCREMENT topic_sentence_count
    RETURN ratio of topic sentences to total paragraphs

FUNCTION is_topic_sentence(sentence):
    RETURN sentence_meets_criteria(sentence)
```

### 2. Presentation
*This metric evaluates the tone, style, and engagement level of the response, ensuring it is appropriate for the context and audience.*

#### Objective
To assess how well the response engages the user through its tone and presentation style.

#### Implementation Details
- **Approach**:
  - Utilize sentiment analysis to determine the tone of the response.
  - Evaluate style metrics to ensure the response is engaging and appropriate.

- **Libraries/Modules**:
  - **TextBlob**: For sentiment analysis.
  - **spaCy**: For style assessment and text classification.

#### Pseudocode:
```sql
FUNCTION evaluate_presentation(reply):
    DETERMINE tone score using tone analysis
    MEASURE engagement score of the reply

    CALCULATE presentation score by averaging tone and engagement scores
    RETURN presentation score

FUNCTION analyze_tone(reply):
    SPLIT reply into sentences
    COUNT positive sentences
    
    CALCULATE tone score based on positive sentences
    RETURN tone score

FUNCTION count_positive_sentences(sentences):
    positive_count = 0
    FOR each sentence IN sentences:
        IF sentence conveys positive tone:
            INCREMENT positive_count
    RETURN positive_count

FUNCTION measure_engagement(reply):
    RETURN calculate_engagement_features(reply)
```

### 3. Cohesion
*This metric measures the degree to which the response maintains focus on the central idea and avoids deviation from the topic.*

#### Objective
To evaluate how well the response stays on topic and maintains coherence throughout.

#### Implementation Details
- **Approach**:
  - Use coherence models to measure semantic similarity within the response.
  - Analyze topic consistency to ensure that the response aligns with the prompt.

- **Libraries/Modules**:
  - **Transformers**: For coherence models.
  - **spaCy**: For entity and coreference analysis.

#### Pseudocode:
```sql
FUNCTION evaluate_cohesion(reply):
    SPLIT reply into sentences
    CREATE pairs of consecutive sentences
    semantic_similarity_sum = 0

    FOR each pair of sentences:
        CALCULATE semantic similarity between the pair
        ADD similarity to semantic_similarity_sum
    
    CALCULATE cohesion score as the average similarity
    RETURN cohesion score

FUNCTION split_into_sentences(reply):
    RETURN reply.split(". ")

FUNCTION create_sentence_pairs(sentences):
    pairs = []
    FOR i IN range(0, len(sentences) - 1):
        ADD (sentences[i], sentences[i + 1]) to pairs
    RETURN pairs

FUNCTION calculate_semantic_similarity(sentence1, sentence2):
    GET embeddings for both sentences
    RETURN cosine similarity between embeddings
```

### 4. Time
*This metric tracks the latency or time taken to generate the response, helping to analyze the efficiency of the chatbot.*

#### Objective
To measure the time taken to process the input and generate the output, aiding in performance evaluation.

#### Implementation Details
- **Approach**:
  - Measure the execution time during the generation of the reply.
  - Log and analyze the time data for performance insights.

- **Libraries/Modules**:
  - **time**: For measuring execution time.
  - **logging**: For storing time-related data.

#### Pseudocode:
```sql
FUNCTION evaluate_time(start_time, end_time):
    CALCULATE response time as the difference
    CALCULATE time score based on response time
    RETURN time score

FUNCTION calculate_time_score(response_time):
    IF response_time < 2:
        RETURN 1.0  # Perfect score for quick responses
    ELSE:
        RETURN inverse scaling of response time  # Lower scores for longer times
```
---

## Evaluation Metrics: Paragraph Level

### Description
This section focuses on the evaluation of paragraphs generated by the system. The metrics assessed at this level include coherence, entity consistency, readability, and richness. These metrics ensure that the generated paragraphs are logically structured, maintain consistency in entities, are readable, and provide lexical richness.

---

### 1. Coherence
*This metric evaluates the logical flow within and between paragraphs, ensuring smooth transitions between ideas and semantic unity across the paragraphs.*

#### Objective
The aim is to measure the logical and semantic connections between consecutive paragraphs to ensure that the text flows smoothly without disjointed thoughts or abrupt transitions.

#### Implementation Details
- **Approach**:
  - Use semantic similarity between consecutive paragraphs to assess coherence.
  - A high similarity between consecutive paragraphs indicates a smooth transition, while a sharp drop indicates a potential coherence issue.
  - Leverage sentence embeddings for calculating similarity scores.

- **Libraries/Modules**:
  - **Transformers**: For sentence embeddings to capture semantic content.
  - **SciPy**: For cosine similarity calculations between embeddings.

#### Pseudocode:
```sql
FUNCTION evaluate_coherence(paragraphs):
    FOR each pair of consecutive paragraphs:
        ENCODE each paragraph into vector representation (e.g., using a language model)
        CALCULATE cosine similarity between paragraph vectors
    RETURN average similarity score across all pairs
```
---

### 2. Entity Consistency
*This metric checks for consistency of entities (such as names, locations, or other important nouns) across paragraphs to avoid contradictions and ensure uniformity in the narrative.*

#### Objective
To measure the consistency of entities across paragraphs by ensuring that the same entities are referenced correctly without errors or contradictions.

#### Implementation Details
- **Approach**:
  - Extract named entities from each paragraph using Named Entity Recognition (NER).
  - Compare entity references across paragraphs to ensure that they are used consistently (e.g., names, places, and dates).
  - Calculate the percentage of consistent entity usage.

- **Libraries/Modules**:
  - **spaCy**: For Named Entity Recognition (NER).
  - **NLTK**: For tokenization and text processing tasks.

#### Pseudocode:
```sql
FUNCTION evaluate_entity_consistency(paragraphs):
    FOR each paragraph:
        EXTRACT named entities using NER (e.g., from a language model)
    FOR each pair of consecutive paragraphs:
        CALCULATE percentage of shared entities between paragraphs
    RETURN average entity overlap score across all pairs
```
---

### 3. Readability
*This metric evaluates the readability of the generated paragraphs based on sentence structure, vocabulary complexity, and clarity.*

#### Objective
To measure how easy it is to read and comprehend the text, focusing on aspects such as sentence length, word complexity, and grammar.

#### Implementation Details
- **Approach**:
  - Calculate readability using standard metrics like the Flesch-Kincaid readability score.
  - Alternatively, for Arabic texts, use a more tailored readability score if available for MSA or specific dialects.

- **Libraries/Modules**:
  - **Textstat**: For readability metrics.
  - **NLTK**: For basic text processing (e.g., sentence length, word complexity).

#### Pseudocode:
```sql
FUNCTION evaluate_readability(paragraphs):
    FOR each paragraph:
        CALCULATE sentence length and complexity using readability formulas
    RETURN average readability score across all paragraphs
```
---

### 4. Richness
*This metric evaluates the lexical diversity and the density of information in each paragraph, ensuring varied and non-repetitive vocabulary.*

#### Objective
To measure the richness of the text by evaluating its lexical diversity and informational content.

#### Implementation Details
- **Approach**:
  - Use lexical diversity metrics (type-token ratio) to measure the variety of vocabulary.
  - Calculate the amount of meaningful content relative to the paragraph length.

- **Libraries/Modules**:
  - **NLTK**: For lexical diversity and word processing.

#### Pseudocode:
```sql
FUNCTION evaluate_richness(paragraphs):
    FOR each paragraph:
        CALCULATE lexical diversity (unique words / total words)
        CALCULATE information density (keywords or important terms per sentence)
    RETURN average richness score combining lexical diversity and information density
```
---

## Evaluation Metrics: Sentence Level

### Description
This section focuses on the evaluation of individual sentences generated by the system. The metrics assessed at this level include grammar, fluency, and verbosity. These metrics ensure that sentences are grammatically correct, fluent, and maintain an appropriate length without excessive verbosity.

### 1. Grammar
*This metric evaluates the grammatical correctness of each sentence, ensuring proper sentence structure, agreement, and use of punctuation.*

#### Objective
The goal is to detect grammatical issues such as incorrect tense, subject-verb agreement, gender/number mismatches, and misplaced punctuation.

#### Implementation Details
- **Approach**:
  - Use a grammar-checking tool to identify and flag errors.
  - Focus on specific issues relevant to Arabic, such as verb-subject agreement and gender/number consistency.
  
- **Libraries/Modules**:
  - **LanguageTool**: For grammar checking.
  - **Farasa**: For Arabic-specific grammar rules and morphological analysis.

#### Pseudocode:
```sql
FUNCTION evaluate_grammar(sentences):
    FOR each sentence:
        RUN grammar check (e.g., using LanguageTool or Farasa)
        IDENTIFY and count grammatical errors
    RETURN total count of errors per sentence and average score
```

### 2. Fluency
*This metric evaluates the fluency of each sentence by measuring how natural the sentence sounds, focusing on perplexity and n-gram models.*

#### Objective
To measure how smoothly the sentence reads, ensuring that the sentence structure and word choice are natural for the language.

#### Implementation Details
- **Approach**:
  - Calculate fluency using perplexity scores or n-gram language models.
  - Use lower perplexity scores as an indicator of higher fluency.

- **Libraries/Modules**:
  - **Transformers**: For calculating perplexity using pre-trained language models.
  - **NLTK**: For building n-gram models and calculating fluency.

#### Pseudocode:
```sql
FUNCTION evaluate_fluency(sentences):
    FOR each sentence:
        CALCULATE perplexity score (e.g., using a pre-trained language model)
    RETURN average perplexity score across all sentences
```

### 3. Verbosity
*This metric evaluates whether the sentence is too verbose or concise, based on its length in relation to the conveyed meaning.*

#### Objective
To measure how well a sentence balances brevity with clarity, avoiding overly long or overly concise sentences that affect understanding.

#### Implementation Details
- **Approach**:
  - Compare sentence length with the amount of meaningful content or specificity.
  - Measure verbosity by calculating the ratio of sentence length to specificity.

- **Libraries/Modules**:
  - **Textstat**: For measuring sentence length.
  - **NLTK**: For evaluating content specificity.

#### Pseudocode:
```sql
FUNCTION evaluate_verbosity(sentences):
    FOR each sentence:
        CALCULATE sentence length (number of words)
        CALCULATE specificity (keywords or important terms per sentence)
        COMPUTE verbosity ratio (length / specificity)
    RETURN average verbosity score across all sentences
```
---

## Evaluation Metrics: Word Level

### Description
This section focuses on the evaluation of individual words in the generated text. The metrics assessed at this level include syntax, grammar, part of speech, and semantics. These metrics ensure that each word is used correctly in terms of its form, meaning, and syntactic role within the sentence.

### 1. Syntax
*This metric evaluates word structure, ensuring that concatenation, hyphenation, and compound word usage conform to proper Arabic syntax.*

#### Objective
To ensure that words are formed and concatenated correctly, respecting Arabic syntax rules such as word boundaries, hyphenation, and compound word formation.

#### Implementation Details
- **Approach**:
  - Analyze the word structure for concatenation and hyphenation errors.
  - Check for valid compound word usage based on Arabic grammar rules.

- **Libraries/Modules**:
  - **Farasa**: For morphological analysis of Arabic words.
  - **PyArabic**: For Arabic word processing and syntax rules.

#### Pseudocode:
```sql
FUNCTION evaluate_syntax(words):
    FOR each word:
        CHECK for syntax errors (e.g., concatenation, hyphenation, compound word formation)
    RETURN total syntax error count and average score
```
---

### 2. Grammar
*This metric evaluates the grammatical form of each word, focusing on correct gender, number, and tense usage.*

#### Objective
To ensure that each word is used in the correct grammatical form, accounting for gender, number, and tense consistency.

#### Implementation Details
- **Approach**:
  - Check the word’s grammatical features (e.g., gender, number, tense).
  - Verify if the word's form matches its role in the sentence context (e.g., subject, verb, or object).

- **Libraries/Modules**:
  - **Farasa**: For grammatical analysis of Arabic words.
  - **NLTK**: For tokenization and tagging of grammatical properties.

#### Pseudocode:
```sql
FUNCTION evaluate_grammar(words):
    FOR each word:
        EXTRACT grammatical properties (e.g., gender, number, tense)
        CHECK if the word's form fits the sentence context
    RETURN total grammatical error count and average score
```
---

### 3. Part of Speech
*This metric evaluates whether the word is used in the correct part of speech within the sentence (e.g., noun, verb, adjective).*

#### Objective
To ensure that the word’s part of speech is appropriate for its position in the sentence, avoiding incorrect usage such as a verb being used in place of a noun.

#### Implementation Details
- **Approach**:
  - Tag the part of speech for each word.
  - Check if the tagged part of speech matches the word’s expected role within the sentence.

- **Libraries/Modules**:
  - **spaCy**: For part of speech tagging.
  - **Farasa**: For Arabic-specific part of speech tagging.

#### Pseudocode:
```sql
FUNCTION evaluate_pos(words):
    FOR each word:
        TAG the part of speech
        CHECK if the word's part of speech is correct for the sentence
    RETURN total part of speech error count and average score
```
---

### 4. Semantics
*This metric evaluates the meaning of the word in the sentence, ensuring that it conveys the intended message without ambiguity or incorrect usage.*

#### Objective
To measure the correctness of the word’s meaning, ensuring that the selected word fits the sentence’s context semantically.

#### Implementation Details
- **Approach**:
  - Calculate semantic similarity between the word and its surrounding context.
  - Check for dissimilar or incorrect word meanings in the sentence.

- **Libraries/Modules**:
  - **Transformers**: For semantic similarity calculations.
  - **NLTK**: For basic text processing and word semantics analysis.

#### Pseudocode:
```sql
FUNCTION evaluate_semantics(words):
    FOR each word:
        CALCULATE semantic similarity with surrounding context
        FLAG words with incorrect or dissimilar meanings
    RETURN total semantic error count and average score
```
---

## Evaluation Metrics: Character Level

### Description
This section focuses on evaluating errors at the individual character level within generated text. Key aspects include character form, ligature handling, and diacritic usage. These errors are critical as they can affect the meaning of words, grammar, and fluency in Arabic.

### 1. Character Forms
*This metric evaluates the correct usage of Arabic characters based on their position within a word—start, middle, end, or isolated form.*

#### Objective
To ensure that Arabic characters appear in their correct forms according to their position in a word. Errors in character forms can lead to confusion or misinterpretation of the word.

#### Implementation Details
- **Approach**:
  - Analyze each character within a word to verify whether its form (start, middle, end, or isolated) is appropriate for its position.
  - Use rules based on Arabic morphology to determine the correct form.
  
- **Libraries/Modules**:
  - **Farasa**: For morphological analysis and word form detection.
  - **NLTK**: For basic tokenization and text processing.

#### Pseudocode:
```sql
FUNCTION evaluate_character_forms(word):
    FOR each character in word:
        DETERMINE its position (start, middle, end, isolated)
        VERIFY if the form matches its position
    RETURN penalty score based on incorrect forms
```

### 2. Diacritics and Harakat (Modifiers)
*This metric checks for the correct usage of diacritics and harakat in Arabic text, as these modifiers can affect pronunciation and meaning.*

#### Objective
To ensure that diacritics, such as harakat, tanween, sukun, and shadda, are used correctly in context. Errors in diacritics can lead to changes in meaning or incorrect pronunciation.

#### Implementation Details
- **Approach**:
  - Detect and verify the correct placement of diacritics (harakat and other modifiers) on the corresponding characters.
  - Distinguish between semantic modifiers (harakat, tanween) that affect meaning and phonetic modifiers (madd, hamzah) that primarily affect pronunciation.
  
- **Libraries/Modules**:
  - **Farasa**: For handling harakat and morphological modifiers.
  - **NLTK**: For basic text tokenization and modifier detection.

#### Pseudocode:
```sql
FUNCTION evaluate_diacritics(word):
    FOR each character in word:
        IF character has a diacritic:
            CHECK if the diacritic is appropriate for the word's meaning and context
    RETURN penalty score for incorrect diacritics
```

### 3. Ligature Handling
*This metric evaluates whether ligatures are used correctly, especially in cases where their presence is necessary or optional in different contexts.*

#### Objective
To ensure that ligatures are used properly in words where required and to check for consistency in usage, particularly between MSA and dialects, where ligature usage may vary.

#### Implementation Details
- **Approach**:
  - Analyze word formations to determine if ligatures are required or used correctly.
  - Compare ligature usage against Modern Standard Arabic (MSA) norms and consider dialect variations where applicable.
  
- **Libraries/Modules**:
  - **Farasa**: For morphological and word-level analysis.
  - **Transformers**: For context-aware checks on ligature usage in generated text.

#### Pseudocode:
```sql
FUNCTION evaluate_ligature_usage(word):
    DETERMINE if the word requires a ligature based on context
    CHECK if the ligature is used correctly according to MSA rules or dialect variations
    RETURN penalty score for incorrect or missing ligatures
```

### 4. Character Semantics
*This metric ensures that the chosen characters, including any associated modifiers, are semantically correct in the context of the sentence.*

#### Objective
To check if the individual characters (including diacritics and ligatures) are semantically appropriate in the word's overall meaning and context within the sentence.

#### Implementation Details
- **Approach**:
  - Analyze the semantic meaning of characters in relation to the sentence.
  - Leverage word embeddings and language models to determine if the correct characters are used based on the surrounding text.
  
- **Libraries/Modules**:
  - **Transformers**: For context-based semantic checks of characters.
  - **NLTK**: For tokenization and basic word processing.

#### Pseudocode:
```sql
FUNCTION evaluate_character_semantics(word, sentence_context):
    FOR each character in word:
        CHECK if the character is semantically correct within the context of the sentence
    RETURN penalty score for incorrect characters or modifiers
```
---

## Asynchronous Scheduler

// Exact design will be specified once it is built as per implementation of evaluation metrics

---

## Output Layer

### Objective:
To generate a comprehensive evaluation report based on the quality metrics applied to the LLM-generated Arabic text, aggregating results, calculating penalties, and providing actionable insights.

### Functionality:

#### 1. Modular Report Generation:
   - Generate individual evaluation results for each level (character, token, word, sentence, etc.).
   - Each module's results include error counts, penalty scores, and other relevant metrics.

#### 2. Aggregated Report:
   - Sum penalties from all levels to compute a total penalty score.
   - Calculate the overall final evaluation score based on the total penalties, using predefined weights for each layer.

#### 3. Error Breakdown:
   - Identify the layers with the highest penalty or error counts (e.g., word-level, sentence-level).
   - Provide specific suggestions for improvement based on the pattern of errors (e.g., grammar fixes, fluency improvements).

#### 4. Report Format:
   - Return the final evaluation report as a structured JSON object.
   - The report includes both individual results for each evaluation module and the overall assessment.

### Pseudocode:
```sql
FUNCTION generate_output_report(evaluation_results):

    INITIALIZE an empty report

    FOR each evaluation layer (character, token, word, sentence, paragraph, response, prompt_reply):
        START asynchronous process to retrieve and compute results from the corresponding evaluation module
        STORE the process handle (future) for each layer

    WAIT for all evaluation processes to complete

    FOR each completed process:
        RETRIEVE the results and STORE in the report under the relevant section (e.g., "character_level", "word_level")

    CALCULATE total penalty by summing penalties from all layers
    CALCULATE final score based on the total penalty and predefined layer weights

    IDENTIFY the layers with the highest error rates or penalties
    GENERATE suggestions for improvement based on the identified errors

    ADD the overall evaluation to the report, including:
        - Total penalty
        - Final score

    ADD error breakdown and suggestions to the report

    RETURN the final report as a JSON object
```
---